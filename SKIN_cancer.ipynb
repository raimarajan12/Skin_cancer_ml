{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "aNgXnZnoPGiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "omhw0c_zPffS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000\n",
        "!unzip -q skin-cancer-mnist-ham10000.zip -d ham10000"
      ],
      "metadata": {
        "id": "XkfGQYYbqNEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIUrRyEDV0rU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"ham10000/hmnist_28_28_RGB.csv\")"
      ],
      "metadata": {
        "id": "PiZvkqKpWsBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,7):\n",
        "  last_column = data.iloc[:, -1]\n",
        "  valid_rows = data[(last_column == 0) | (last_column == i)]\n",
        "  valid_rows.to_csv(f'{i}.csv', index=False)"
      ],
      "metadata": {
        "id": "dYK5PyzsW-aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "for i in range(1,7):\n",
        "  fname=str(i)+'.csv'\n",
        "  data=pd.read_csv(fname)\n",
        "  print(data.shape)\n",
        "  print(data[data.columns[-1]].value_counts())\n",
        "  data=data.iloc[:,:]\n",
        "  data=np.array(data)\n",
        "  np.savetxt(fname,data,delimiter=',', fmt='%f')"
      ],
      "metadata": {
        "id": "Lsj7cBKRXYEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats"
      ],
      "metadata": {
        "id": "9nLo83olyrli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigtestfinal(j):\n",
        "    fname=f\"{j}.csv\"\n",
        "    df=np.genfromtxt(fname,delimiter=',')\n",
        "    num_features = df.shape[1] - 1\n",
        "    p_values = np.zeros(num_features)\n",
        "    y=df[:, -1]\n",
        "    in0=np.where(y==0)\n",
        "    in1=np.where(y>0)\n",
        "    for i in range(num_features):\n",
        "        feature_values = df[:, i]\n",
        "        _, p_values[i] = stats.ranksums(feature_values[in0], feature_values[in1])\n",
        "    return p_values,df"
      ],
      "metadata": {
        "id": "uTdzOw99XvqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "impf=[]\n",
        "impf1=[]\n",
        "for i in range(1,7):\n",
        "    p,df=sigtestfinal(i)\n",
        "    sigf=np.where(p<=0.05)\n",
        "    impf.append(sigf[0])\n",
        "    print(i,len(sigf[0]))\n",
        "    data=np.concatenate((df[:,sigf[0]],df[:,-1].reshape(-1,1)), axis=1)\n",
        "    fname=str(6+i)+'.csv'\n",
        "    np.savetxt(fname,data, delimiter=',', fmt='%f')"
      ],
      "metadata": {
        "id": "UeOe_vJAXxoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "for i in range(1,7):\n",
        "    fname=str(i)+'.csv'\n",
        "    df=np.genfromtxt(fname,delimiter=',')\n",
        "    pca = PCA(n_components=0.99)\n",
        "    pca.fit(df[:,0:-1])\n",
        "    data=pca.transform(df[:,0:-1])\n",
        "    data=np.concatenate((data,df[:,-1].reshape(-1,1)), axis=1)\n",
        "    print(i,np.shape(data))\n",
        "    fname=str(12+i)+'.csv'\n",
        "    np.savetxt(fname,data, delimiter=',', fmt='%f')"
      ],
      "metadata": {
        "id": "D7yNnAXGYuZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import svm"
      ],
      "metadata": {
        "id": "XRLnB-PaY_Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fmodel(trdata):\n",
        "  model=[]\n",
        "  model.append(MultinomialNB())\n",
        "  model.append(BernoulliNB())\n",
        "  model.append(GaussianNB())\n",
        "  model.append(DecisionTreeClassifier())\n",
        "  model.append(LogisticRegression())\n",
        "  model.append(KNeighborsClassifier())\n",
        "  model.append(svm.SVC(kernel=\"linear\",probability=True))\n",
        "  model.append(svm.SVC(kernel=\"poly\",probability=True))\n",
        "  model.append(svm.SVC(kernel=\"rbf\",probability=True))\n",
        "  model.append(BaggingClassifier(MultinomialNB(),max_samples=0.5, max_features=0.5))\n",
        "  model.append(BaggingClassifier(DecisionTreeClassifier(),max_samples=0.5, max_features=0.5))\n",
        "  model.append(BaggingClassifier(LogisticRegression(),max_samples=0.5, max_features=0.5))\n",
        "  model.append(BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5))\n",
        "  model.append(RandomForestClassifier(n_estimators=10))\n",
        "  model.append(ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0))\n",
        "  model.append(AdaBoostClassifier(n_estimators=10))\n",
        "  model.append(GradientBoostingClassifier(n_estimators=10, learning_rate=1.0,max_depth=1, random_state=0))\n",
        "  model.append(MLPClassifier(solver='lbfgs', hidden_layer_sizes=(trdata.shape[1], 2), random_state=1,max_iter=1500))\n",
        "  model.append(MLPClassifier(solver='sgd', hidden_layer_sizes=(trdata.shape[1], 2), random_state=1,max_iter=1500))\n",
        "  model.append(MLPClassifier(solver='adam', hidden_layer_sizes=(trdata.shape[1], 2), random_state=1,max_iter=1500))\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "m55ri856ZBSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "def normalizedata(X_train):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train = scaler.transform(X_train)\n",
        "    return X_train"
      ],
      "metadata": {
        "id": "EiP7SISVZDGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(3)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import time\n",
        "import os\n",
        "\n",
        "def fmodel(x):\n",
        "    model = []\n",
        "    model.append(GaussianNB())\n",
        "    model.append(LogisticRegression(max_iter=1000))\n",
        "    model.append(KNeighborsClassifier())\n",
        "    return model\n",
        "\n",
        "def normalizedata(X):\n",
        "    return (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0) + 1e-8)\n",
        "\n",
        "\n",
        "\n",
        "for i in range(1,19):\n",
        "    start = time.time()\n",
        "    print(f\"\\nProcessing Dataset {i}\")\n",
        "    fname=f\"{i}.csv\"\n",
        "    data=np.genfromtxt(fname,delimiter=',')\n",
        "    np.random.shuffle(data)\n",
        "    predvalue=np.zeros((data.shape[0],21))\n",
        "    predvalue1=np.zeros((data.shape[0],21))\n",
        "    data[:,:-1]=normalizedata(data[:, :-1])\n",
        "    for fold_idx, (train_index, test_index) in enumerate(kf.split(data)):\n",
        "        trdata = data[train_index, :-1]\n",
        "        tsdata = data[test_index, :-1]\n",
        "        tract = data[train_index, -1]\n",
        "        tsact = data[test_index, -1]\n",
        "\n",
        "        if len(tract.shape) > 1:\n",
        "            tract = tract.ravel()\n",
        "        tract = tract.astype(np.float64)\n",
        "        model=fmodel(trdata)\n",
        "        for j in range(len(model)):\n",
        "            mod = model[j]\n",
        "            try:\n",
        "                model_start = time.time()\n",
        "                mod.fit(trdata, tract)\n",
        "                predvalue[test_index, j] = mod.predict(tsdata)\n",
        "\n",
        "                try:\n",
        "                    predvalue1[test_index, j] = mod.predict_proba(tsdata)[:, 1]\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                print(f\"✅ Finished model {j+1}/{len(model)} for dataset {i} on fold {fold_idx+1}\")\n",
        "                print(f\"⏱ Model {j+1} took {time.time() - model_start:.2f} sec\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Model {j+1} failed on dataset {i}: {e}\")\n",
        "\n",
        "\n",
        "        predvalue[test_index,20]=tsact\n",
        "        predvalue1[test_index,20]=tsact\n",
        "    \"\"\"np.savetxt(\"pred.csv\",predvalue, delimiter=',', fmt='%f')\n",
        "    np.savetxt(\"predp.csv\",predvalue1, delimiter=',', fmt='%f')\"\"\"\n",
        "    np.savetxt(f\"{i}pred.csv\", predvalue, delimiter=',', fmt='%f')\n",
        "    np.savetxt(f\"{i}predp.csv\", predvalue1, delimiter=',', fmt='%f')\n",
        "    print(f\"✅ Finished Dataset {i} in {time.time() - start:.2f} seconds\")"
      ],
      "metadata": {
        "id": "eODGbXuWZGK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZVBmBGaBVyTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import (\n",
        "    f1_score, precision_score, recall_score,accuracy_score\n",
        "    )"
      ],
      "metadata": {
        "id": "5azEkRs3ZVEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "acv=np.zeros((72,20))\n",
        "for i in range(0,18):\n",
        "    fname= str(i+1)+\"pred.csv\"\n",
        "    data1=np.genfromtxt(fname,delimiter=',')\n",
        "    y1=data1[:, -1]\n",
        "    unique_labels = np.unique(y1)\n",
        "    if len(unique_labels) != 2:\n",
        "        print(f\"⚠️ Skipping dataset {i+1}: not binary labels: {unique_labels}\")\n",
        "        continue\n",
        "    label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
        "    y1 = np.vectorize(label_map.get)(y1)\n",
        "    for j in range(0,20):\n",
        "        pred_mapped = np.vectorize(label_map.get)(data1[:, j])\n",
        "        acv[i, j] = accuracy_score(y1, pred_mapped)\n",
        "        acv[18+i,j]=precision_score(y1, pred_mapped)\n",
        "        acv[36+i,j]=recall_score(y1, pred_mapped)\n",
        "        acv[54+i,j]=f1_score(y1, pred_mapped)\n",
        "fname='acc.csv'\n",
        "np.savetxt(fname,acv, delimiter=',', fmt='%f')"
      ],
      "metadata": {
        "id": "mXAR8EPKZXie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "rocv=np.zeros((18,20))\n",
        "for i in range(0,18):\n",
        "    fname= str(i+1)+\"pred.csv\"\n",
        "    data=np.genfromtxt(fname,delimiter=',')\n",
        "    y1=data[:,-1]\n",
        "    unique_labels = np.unique(y1)\n",
        "\n",
        "    if len(unique_labels) != 2:\n",
        "        print(f\"Skipping dataset {i+1}: not binary labels: {unique_labels}\")\n",
        "        continue\n",
        "\n",
        "\n",
        "    label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
        "    y1 = np.vectorize(label_map.get)(y1)\n",
        "    for j in range(0,20):\n",
        "        b = np.vectorize(label_map.get)(data[:, j])\n",
        "        fpr, tpr, _ = roc_curve(y1, b)\n",
        "        rocv[i,j]=auc(fpr, tpr)\n",
        "fname='auc.csv'\n",
        "np.savetxt(fname,rocv, delimiter=',', fmt='%f')"
      ],
      "metadata": {
        "id": "AfIwYvxAZixN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/raimarajan12/skin_cancer_ml.git\n",
        "%cd  skin_cancer_ml\n",
        "!mv /content/SKIN_cancer.ipynb /content/skin_cancer_ml/\n",
        "from getpass import getpass\n",
        "token = getpass(\"Enter your GitHub token: \")\n",
        "\n",
        "!git config --global user.email \"rajanraima12@email.com\"\n",
        "!git config --global user.name \"raimarajan12\"\n",
        "\n",
        "!git add SKIN_cancer.ipynb\n",
        "!git commit -m \"Initial push of clean notebook\"\n",
        "push_url = f\"https://{token}@github.com/raimarajan12/skin_cancer_ml.git\"\n",
        "!git push {push_url}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw1AFvqljYeJ",
        "outputId": "bd74f474-679f-4dcb-a2f3-5b69c9e8dd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "fatal: could not create work tree dir 'skin_cancer_ml': No such file or directory\n",
            "[Errno 2] No such file or directory: 'skin_cancer_ml'\n",
            "/content/skin_cancer_ml\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "mv: cannot stat '/content/SKIN_cancer.ipynb': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iyt1WlQxlwPJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}